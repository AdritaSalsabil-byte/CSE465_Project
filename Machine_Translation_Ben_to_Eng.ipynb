{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Translation_Ben_to_Eng.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt",
        "colab_type": "text"
      },
      "source": [
        "Neural machine translation, Bangla to English\n",
        "We have trained a sequence to sequence model to translate Bangla words and short texts to English\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa",
        "colab_type": "text"
      },
      "source": [
        "We are using a Bengali-English dataset from anki."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIlo1fucWHk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dGt4X7DWY5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/drive/My Drive/Datasets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GkHxI-1Wgc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# path_to_file = os.path.dirname(path_to_zip)+\"/ben-eng/ben.txt\"\n",
        "corpus_name = \"ben\"\n",
        "corpus = os.path.join(\"/content/drive/My Drive/Datasets/\", corpus_name)\n",
        "\n",
        "path_to_file = os.path.join(corpus, \"ben.txt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNRl3MMqX-1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "01c23954-540a-4c38-d8bd-d5e1c852c706"
      },
      "source": [
        "def printLines(file, n=15):\n",
        "    with open(file, 'r', encoding='utf-8') as path_to_file:\n",
        "        lines = path_to_file.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "        \n",
        "printLines(path_to_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tযাও।\n",
            "\n",
            "Go.\tযান।\n",
            "\n",
            "Go.\tযা।\n",
            "\n",
            "Run!\tপালাও!\n",
            "\n",
            "Run!\tপালান!\n",
            "\n",
            "Who?\tকে?\n",
            "\n",
            "Fire!\tআগুন!\n",
            "\n",
            "Help!\tবাঁচাও!\n",
            "\n",
            "Help!\tবাঁচান!\n",
            "\n",
            "Stop!\tথামুন!\n",
            "\n",
            "Stop!\tথামো!\n",
            "\n",
            "Stop!\tথাম!\n",
            "\n",
            "Hello!\tনমস্কার!\n",
            "\n",
            "I see.\tবুঝলাম।\n",
            "\n",
            "I try.\tআমি চেষ্টা করি।\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd0jw-eC3jEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the unicode file to ascii\n",
        "def unicode_to_ascii(string, accents=('COMBINING ACUTE ACCENT', 'COMBINING GRAVE ACCENT', 'COMBINING TILDE')):\n",
        "    accents = set(map(unicodedata.lookup, accents))\n",
        "    chars = [c for c in unicodedata.normalize('NFD', string) if c not in accents]\n",
        "    return unicodedata.normalize('NFC', ''.join(chars))\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  w = re.sub(r\"([.!?])\", r\" \\1\", w)\n",
        "  w = re.sub(r\"\\s\", r\" \", w).strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOxr1SlmZIFE",
        "colab_type": "text"
      },
      "source": [
        "#Process Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opI2GzOt479E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b4330ced-3617-406c-fc18-fcc12081f079"
      },
      "source": [
        "bn_sentence = u\"আমি চেষ্টা করেছিলাম।\"\n",
        "en_sentence = u\"I tried.\"\n",
        "print(preprocess_sentence(bn_sentence))\n",
        "print(preprocess_sentence(en_sentence))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> আমি চেষ্টা করেছিলাম। <end>\n",
            "<start> i tried . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHn4Dct23jEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, BANGLA]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTbSbBz55QtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "434cc296-5f32-4423-d79a-2980ea26336c"
      },
      "source": [
        "bn, en = create_dataset(path_to_file, None)\n",
        "print(bn[-1])\n",
        "print(en[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> i thought doing this would be easy, but we've been working all day and we're still not finished . <end>\n",
            "<start> আমি ভেবেছিলাম এটা করা সহজ হবে, কিন্তু আমরা সারাদিন ধরে কাজ করেছি আর এখনো শেষ করে উঠতে পারিনি। <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDUi1_HPyJbr",
        "colab_type": "text"
      },
      "source": [
        "We will use keras Tokernizer to split our texts into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOn8RCNDJXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEyJeE8dyccH",
        "colab_type": "text"
      },
      "source": [
        "Here we will create cleaned input, output pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOi42V79Ydlr",
        "colab_type": "text"
      },
      "source": [
        "Experiment with size of dataset \n",
        "\n",
        "We tried to experiment with the size of the dataset to see different results. Using smaller portion of dataset led to faster computation but lower translation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnxC7q-j3jFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# experimenting with the size of dataset\n",
        "num_examples = 5000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Pe6XIKzRn_",
        "colab_type": "text"
      },
      "source": [
        "###We created training and validation sets using an 80-20 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QILQkOs3jFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f61a524-bc6b-452d-ebf7-4de883ca8a9d"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3473 3473 869 869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXukARTDd7MT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "95db7665-593f-4926-9723-189b13f4c241"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> আমি\n",
            "18 ----> তোমার\n",
            "149 ----> উপর\n",
            "520 ----> নির্ভর\n",
            "14 ----> করে\n",
            "106 ----> আছি।\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "5 ----> i\n",
            "75 ----> am\n",
            "606 ----> counting\n",
            "53 ----> on\n",
            "7 ----> you\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d",
        "colab_type": "text"
      },
      "source": [
        "Creating a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc6-NK1GtWQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7a02358-c713-44c2-f565-ed788e2bf607"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 20]), TensorShape([64, 22]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu",
        "colab_type": "text"
      },
      "source": [
        "###Forming the encoder and decoder model\n",
        "\n",
        "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "\n",
        "\n",
        "* FC = Fully connected (dense) layer\n",
        "* EO = Encoder output\n",
        "* H = hidden state\n",
        "* X = input to the decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gSVh05Jl6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3aad84cc-fbf1-40cb-e86f-aa092496d7d9"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 20, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umohpBN2OM94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k534zTHiDjQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79061928-270d-4035-8cac-f54afade9bc2"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb900956-d452-4e6f-ec7e-bc04ea2adbb2"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 1961)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK",
        "colab_type": "text"
      },
      "source": [
        "Defining the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e",
        "colab_type": "text"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU",
        "colab_type": "text"
      },
      "source": [
        "Training the model\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "b64fb905-dd94-432e-9721-2f6e6b917bfa"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2884\n",
            "Epoch 1 Loss 1.5394\n",
            "Time taken for 1 epoch 372.8184766769409 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2817\n",
            "Epoch 2 Loss 1.2409\n",
            "Time taken for 1 epoch 342.7709515094757 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1074\n",
            "Epoch 3 Loss 1.1223\n",
            "Time taken for 1 epoch 345.53867053985596 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0647\n",
            "Epoch 4 Loss 1.0185\n",
            "Time taken for 1 epoch 347.7530131340027 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.8557\n",
            "Epoch 5 Loss 0.9285\n",
            "Time taken for 1 epoch 346.1806833744049 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8486\n",
            "Epoch 6 Loss 0.8450\n",
            "Time taken for 1 epoch 349.71933579444885 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7701\n",
            "Epoch 7 Loss 0.7703\n",
            "Time taken for 1 epoch 347.1187114715576 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7508\n",
            "Epoch 8 Loss 0.6986\n",
            "Time taken for 1 epoch 351.3354160785675 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz",
        "colab_type": "text"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5hQWlbN3jGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9zUHzg3jGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP",
        "colab_type": "text"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpT9D5_OgP6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "525195b4-5599-4349-878a-e4a959359938"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4c6f0ac8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrAM0FDomq3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34bff57c-64d1-4d18-d9ef-fef2c4eca9a8"
      },
      "source": [
        "translate(u'তুমি কি এখন খেতে চাও কিছু জানানোর আছে')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> তুমি কি এখন খেতে চাও কিছু জানানোর আছে <end>\n",
            "Predicted translation: drink sick ? <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2468 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2497 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2495 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2453 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2447 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2454 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2472 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2503 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2458 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2451 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2459 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2460 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2507 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2480 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2438 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2468 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2497 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2495 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2453 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2447 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2454 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2472 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2503 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2458 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2451 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2459 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2460 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2507 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2480 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2438 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSx2iM36EZQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "529dcb23-d8b3-4ba9-bb89-0a45a4f47312"
      },
      "source": [
        "translate(u'তুমি কি পুরোটা পরেছো?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> তুমি কি পুরোটা পরেছো ? <end>\n",
            "Predicted translation: did you read it all ? <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2468 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2497 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2495 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2453 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2474 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2480 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2507 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2463 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2503 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2459 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2468 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2497 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2495 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2453 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2474 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2480 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2507 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2463 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2503 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2459 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAJwCAYAAAC08grWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5htd13f8c83ObmYBFAQMFAiCKIg3uAQoEFEgxcUWxEeqlwjllgveCtqpfV+aVW00moLQREDKiiWBkRRMCIXtQiINYBgFCJIIaQiIRFy/faPvU+ZPZyTzBzOme+emdfreXgys/aame/8nk32O2utvaa6OwAAE06YHgAA2L+ECAAwRogAAGOECAAwRogAAGOECAAwRogAAGOECAAwRogAAGOECAAwRoisgar69Kq6uKo+e3oWANhJQmQ9PCHJg5M8cXgOANhR5Y/ezaqqSvLOJC9P8lVJ7tDdN4wOBQA7xBGReQ9Ocosk35bk+iRfMToNAOwgR0SGVdVzklzb3edX1c8k+dTufuTwWDCuqn4qySdv40ve0d0/erzmAY4PITKoqk5P8n+SfGV3v7qqPi/JnyQ5s7v/cXY6mFVVb0ryqCS1ld2TXNjdZx/fqYBj7cD0APvcI5Jc0d2vTpLuflNV/XWSr03yjNHJYF5399u3uvPyeitgafkfu49IclF3f3B6niNxjcisxyV53qZtz0ty3s6PAmtnu4drHd6FVY9K8stZvNasLSEypKrulOSLkjx300O/luRgVd1956cCYA95fJK3Zc3/49apmSHd/a4cZv27+92H2w4AW1VVd05yTpKzk/xpVd2zu98yOtQReMEbVFVnJXlXH+aK4ao6q7v/bmAsWBenVNXjt7hvZWsXtcJ+8bgkr15ee/g7Wdw483uHZzos75oZVFU3ZPEOmcs3bb9Nksu7+8SZyWBeVT06i3vsbNXl3f2i4zUP7CbLNz78eHc/p6oekeTpSe50uP/wneaIyKzK4S+wOyPJR3Z4Flg3lyY5dRv7f+h4DbIOqursbHM9uvvPj9c8rK+q+udJzkzywuWmlyR5VpKHZHEX77XiiMiAqvovyw+/JYsrmv9pw8MnZnFO79ruPmenZ4N1UVVvSfI/s/VTLufu5fuIWA+2qqqemeSM7n7Mhm3PSHKLjdvWhSMiMw79ld1Kco8k12547Nokb0zytJ0eCtbMNd391K3uXFV/djyHWQPWg5tVVadk8bbdr9v00POS/F5VndHdV+38ZEcmRAZ09xctb770G0me2N17+pAyHCX3EVllPdiKWyT59iS/v3Fjd7+mqr4xi1P/axUi7iMy54QkX53kTtODALA3dPcV3X1hd994mMee193vnZjrpgiRId19Q5LLkpw8PQsATHFqZtaPJvlPVfXY7r5iehjY5dxHZJX12Eeq6h3Z4um47v604zzOtgiRWU9Jcpckf19V705y9cYHu/tzRqaC9XBZVf3JNvb/y+M2yXqwHtyUn9/w8RlJvivJ67L4i+5J8oAs3pH5Mzs8183y9t1BVfWDN/V4d//wTs0CwN5QVc9J8vbu/olN278vyWd192NHBjsCIcKoqnpltn6dTCV5b3c//PhNxLrY5nMjSd63l58b1oOtqqork9y7uy/dtP1uSd7Y3becmezwnJph2q26+/O3urN7I+wrnhurrAdbdXWSB2dxd+KNHpzVG2iuBSEyqKpOTvLvs7jxzFlJTtr4+D75WzPujcCReG6ssh5s1X9O8gtVdTDJny633T+LP3z3Q1NDHYkQmfWjSf5Vkv+YxRPnu5PcOcnXJvn+ubEA2K26+6eq6p1Z3NjsUcvNb03yhO7+jbHBjkCIzHpUkn/T3S+rqqcluai7/6aq3prkS5I8c3Y8AHajZXCsXXQcjhCZdfskb1l+fFWST1x+/LIkPzkyEQB7RlV9YjbdvLS7/2FonMMSIrP+Lskdlv+8NMmXJXlDFu/3/vDgXDvp9Kp69hb3rbhJ037iubHKerAlVfWpSZ6RxcWpG99pVVlcO7RW1x8KkVkvSnJuFhcTPT3Jr1fVk5LcMclPTw62gx6aTRfp3oz9Emh4bmxmPdiqX87iCPs3JHlP1vzCZfcRWSNVdb8k52RxI5rfnp5nJ1TVt+Wjp6S24j3d/YvHax7Wh+fGKuvBVlXVVUnu392XTM+yFUJkUFU9KMkfd/f1m7YfSPLPu/tVM5PtnKr631nc6n6rh5F/tLvPPo4jsSY8N1ZZD7aqqv4yyXnd/YbpWbZCiAyqqhuSnNndl2/afpskl++H+4hU1Z9v9yZN3X3f4zkT68FzY5X1YKuq6ouT/Lsk37z57qrryDUisw5dOLTZbbLpD+DtYW7SxJF4bqyyHmzVRUlOSfK2qromycpRd7d4J1X14uWHneR5yyfKIScmuVeSP97xwQDYC751eoDtECIz/u/yn5XkA1m9uv3aJK9J8qydHgqA3a+7f2V6hu0QIgO6++uTZHkL3qd19345DXM4Jy0v2t0K90bYXzw3VlkPtqyqbp/kcUnumuT7u/uKqjoni3dTvWN2ulUuVh1UVSckSXffuPz8U5I8LMlbuntfnJqpqu9J8knb+JJ3d/cvHK95WB+eG6usB1tVVfdJ8gdJ3pHks5J8Znf/bVX9UJK7d/ejJ+fbTIgMqqrfTfKy7n56VZ2R5K+SnJ7kjCTf0N0Xjg64A6rqDtnekblruvt9x2ueSdZilfVYZT3Yqqr6wySv6u4frKoPJfncZYg8IMnzu/tTh0dc4dTMrINJvmf58dckuTLJXZI8Jov7Bez5EElycZI35sjvINqosjjMuFfvjbBxLbZiL69F4rmxmfVgq+6TxV1VN/s/WfyNs7UiRGadkeQflx9/aZIXdfd1VXVxkv1ySPXD2zlMWFV/djyHGWYtVlmPVdaDrfpwDn8a7zOTXH6Y7aNOuPldOI7+Lsk5VXV6Fn/w7uXL7bdO8k9jU+0s90b4KGuxynqssh5s1UVJfrCqTll+3lV15yz+qvtvTQ11JEJk1s8meW6Sdyf5+ySHbun+oCR/OTUUALvaU7L4D9r3Jzkti1tCXJrkg0n+w+Bch+XUzKDufmZVvT7JWUlefujdM0n+Jsn3z00GwG7V3VcmeeDyVu/3zuKgwxu7+xWzkx2eEBlSVbdK8jnd/eokm/8w0T8mecvOT7UruDfCR1mLVdZjlfXYhza+tnT3xVlc5HzosXOyuD3EB8YGPAwhMufGJL9bVV/W3a89tLGqPjeLJ84dxybbWddW1XbumfL+4zbJPGuxynqssh5sxa57bREiQ7r7Q1V1UZLHJ3nthocel+T3uvuKmcl23DuSfMo29r/seA2yBqzFKuuxynpws3bja4sQmXVhkl+vqid397XLO60+OrvsDxZ9nD4jyf2ztcPIlY9e0LsXWYtV1mOV9WCrdtVrixCZ9fIs3u/9sCT/I8m5SU5O8pLJoXZYdfe1W965ai+f97YWq6zHKutxGFX1sCR3y+KOoe+dnmdN7KrXFm/fHbR8l8zzsjiEliwOnb2gu6+bm2rHuTfCR1mLVdZjlfXYpKr+XZIXJfnuJH9RVZ89PNJa2G2vLUJk3oVJvryqzkry8CS76s83Awz65iz+Ltcdkzw9ycur6kur6qyqOlBVZy7/3bof7ZrXFqdmhnX3m6vqkiS/msVfy3zd9EwAu8Sts7wWprt/YnktxO8uH7tvFv9evXuSE2fGm7ObXluEyHq4MMnPJfn304MM+ISq+oEt7rvXz3lbi1XWY5X1+FhvT3LPJO9Mku7+sar6pSRnJnlrFqcmThubbt6ueG2p7j1/GnHtVdWtkzw5yTP328VWVfWgJJ+wjS/5YHf/6fGaZ5K1WGU9VlmPj1VV35rki7r7EdOzrKPd8toiRACAMS5WBQDGCBEAYIwQWRNVdf70DOvEeqyyHqusxyrrscp6rFr39RAi62OtnygDrMcq67HKeqyyHqusx6q1Xg8hAgCM2ffvmjm5TulTc/r0GLku1+SknDI9xtqwHqusxyrrscp6rLIeq9ZlPT6UD1zR3bfdvH3f39Ds1Jye+9W502MAsNecsO9u6HqTXnHDCy473HanZgCAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMUIEABgjRACAMbsmRKrqt6vqOcuPX1lVP38z+19SVT+0E7MBAEfnwPQAR+lrklw3PQQA8PHZlSHS3f8wPQMA8PFby1MzVXVaVT2nqq6qqvdV1VM3Pb5yaqaqbldVF1XVh6vqsqp64s5PDQBs11qGSJKnJfmSJI9Icm6Sz0/yoJvY/zlJ7pbkIUm+Osnjk9z5uE4IAHzc1u7UTFWdkeQbkjyxu39vue3rk7z7CPvfPclDkzywu1+73PaEJH97Ez/j/CTnJ8mpOe2Yzg8AbN06HhG5a5KTk/zJoQ3dfVWSvzzC/vdIcmOS123Y/7Ik7znSD+juC7r7YHcfPCmnHJOhAYDtW8cQOVo9PQAAsD3rGCJ/k8Vbc+9/aENVnZ7kXkfY/6+y+D3O3rD/WUnucBxnBACOgbW7RqS7r6qqX0ryk1X1/ixOsfxAkhOPsP/bquplSZ65vPbjw0l+dvlPAGCNrV2ILD0lyelJXpTkn5L81+XnR3JekmcluTjJFUl+OMntju+IAMDHq7r396UVt6xb9/3q3OkxANhrTjjsgfx96xU3vOAN3X1w8/Z1vEYEANgnhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMEaIAABjhAgAMObA9AAAsBfViSdOj7Bebjj8ZkdEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGCNEAIAxQgQAGDMeIlX1+Kr6v1V1yqbtv1pVL15+/I1VdWlVXbv855M27dtV9chN295ZVU85/r8BAHC0xkMkyW9mMce/PLShqm6V5OFJfqmqHp7k55P8XJJ7JXl6kv9WVV81MCsAcAwdmB6guz9cVb+a5IlJfmO5+dFJrkzy0iR/lOS53f3zy8feXlX3SfK9SV5yND+zqs5Pcn6SnJrTPo7pAYCPxzocEUmSZyX5kqr6Z8vPn5jkV7r7+iT3SPLaTfu/Jsk9j/aHdfcF3X2wuw+elFNu/gsAgONiLUKku/8iyRuTnFdV90pyMMmzb+7LNn1cmx4/6dhNCAAcD2sRIkvPSnJekn+d5LXd/bbl9rcmOWfTvg9M8pYNn78/yZmHPqmq22/8HABYT+PXiGzw60l+Nsk3Jfk3G7b/dJLfrKo3JPn9JF+e5DFJvmbDPhcn+Zaq+uMkNyT5iSQf2YmhAYCjtzZHRLr7Q1lcrHpNPnrRarr7fyZ5cpLvzOIoyLcn+ebu3nih6r9N8rdJXpnkhUl+McnlOzI4AHDU1umISLI4nfKC7r5648bufkaSZxzpi7r7PUkeumnzbx378QCAY2ktQqSqPinJFyT50iSfOzwOALBD1iJEkvx5klsneWp3XzI9DACwM9YiRLr7ztMzAAA7b20uVgUA9h8hAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwBghAgCMESIAwJgD0wOshRNOnJ5gffSN0xOsl+7pCdaL/6+sqBOtx0Z9ww3TI6yVl132uukR1sqJZx5+uyMiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMAYIQIAjBEiAMCYPRsiVXVVVZ03PQcAcGR7NkQAgPU3GiJVdfLkzwcAZu1oiFTVK6vqv1fV06rq/UleW1X3rKqXVtWHquryqvr1qvqUDV9z36r6/aq6oqqurKrXVNUDNn3fuy2/90eq6m1V9bCd/L0AgKMzcUTksUkqyRck+bYkr0pySZKzkzwkyRlJLqqqQ7PdIslzl/ufneRNSX6nqm6TJMv9XpTF7/KAJE9M8kNJTtmZXwcAOFoHBn7mO7r73yZJVf1Ikr/o7u899GBVPT7JPyQ5mOR13X3xxi+uqicneUSShyZ5Xhbxcs8kd+nuv1vu8x1JXn2kAarq/CTnJ8mpOe3Y/WYAwLZMHBF5w4aP75PkQct3uFxVVVcledfysbsmSVXdrqqeWVVvr6oPJvlQktslOWu53z2S/P2hCFn6X0luPNIA3X1Bdx/s7oMnOXACAGMmjohcveHjE5K8NMlTDrPf+5b//JUkt0/ynUnemeSaJH+QxIWuALDLTYTIRm9M8qgkl3X3dUfY54FJvq27X5okVXX7JGduePytSe5YVXfq7kNHU86OtyYDwNqbfrH+hSS3SvKCqrpfVX1aVT2kqi6oqlss93l7kscu311z3yTPT3Lthu/xiiR/leTCqvq85Ttq/nOS63fw9wAAjsJoiHT3e5Kck8X1HC9L8uYs4uSa5f+Sxbtgzsji2pLnJ3l2FqdoDn2PG5M8PIvf5X8luTDJj234egBgTe3oqZnufvBhtv11kkfexNf8RZL7bdr83E37vD3JF27a54yjmxIA2CnTp2YAgH1MiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAYw5MDzCtTjghJ3zCqdNjrI068cTpEdZK33DD9AhrpU7a9//KWNHXXT89wlrpq6+dHmGtfNkdPm96hDVz6WG3OiICAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIzZEyFSVc+pqt+engMA2J4D0wMcI9+epJKkql6Z5JLu/tbRiQCAm7UnQqS7Pzg9AwCwfXsiRKrqOUk+OckVSb4wyRdW1bcsH75Ld79zaDQA4CbsiRDZ4NuT3D3JXyV56nLb++fGAQBuyp4Kke7+YFVdm+Sfuvu9R9qvqs5Pcn6SnFqn79R4AMAme+JdM9vV3Rd098HuPnhynTo9DgDsW/syRACA9bAXQ+TaJCdODwEA3Ly9GCLvTHJ2Vd25qj65qvbi7wgAe8JefJF+WhZHRd6SxTtmzpodBwA4kj3xrpnuPm/Dx29P8oC5aQCArdqLR0QAgF1CiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAY4QIADBGiAAAYw5MDzCtb7wxN1599fQYALAvOSICAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIzZMyFSVV1VjzzS5wDA+tkzIQIA7D5CBAAYs2tCpKq+vKpeXVUfqKp/qKrfq6p7TM8FABy9XRMiSU5P8nNJzk7y4CQfTPKSqjp5u9+oqs6vqtdX1euvyzXHdkoAYMsOTA+wVd39Wxs/r6qvT3JlFmHymm1+rwuSXJAkt6xb97GaEQDYnl1zRKSq7lpVv1ZVf1NVVyZ5XxbznzU8GgBwlHbNEZEkv53k3Um+McnfJ7k+yVuSbPvUDACwHnZFiFTVbZJ8ZpJv7u4/XG67d3bJ/ADA4e2WF/IPJLkiyZOq6l1J7pjkp7M4KgIA7FK74hqR7r4xyb9K8jlJLknyC0m+P/GWFwDYzXbLEZF098VJ7rVp8xkbHq9N+1cAgLW2K46IAAB7kxABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYcmB5gLVRNT7A26sQTp0dYK33DDdMjsM66pyeAXc8REQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMYIEQBgjBABAMbsqRCpqm+tqj+vqqur6l1V9X3TMwEAR3ZgeoBj7NwkP5DkzUkelOQXq+rN3f3i2bEAgMPZUyHS3Q/f8OnfVtVPJLnb1DwAwE3bUyGyUVU9NclJSZ5/mMfOT3J+kpya03Z4MgDgkD11jcghVfUfknxHki/p7vdsfry7L+jug9198KScsvMDAgBJ9uARkaq6Q5IfSfKV3f2m6XkAgCPbi0dEzkxSSd46PQgAcNP2Yoi8Ncl9k3zMKRkAYL3sxRC5V5LnJbnt9CAAwE3biyFyWpLPyOIdMwDAGttzF6t29yuzuEYEAFhze/GICACwSwgRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGDMgekB1kL39ARro6+/fnoEAPYRR0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDFCBAAYI0QAgDG7JkSq6ilV9c7pOQCAY2fXhAgAsPcckxCpqltW1Scei++1jZ9526o6dSd/JgBwbB11iFTViVX1ZVX1a0nem+Rzl9tvVVUXVNXlVfWhqvqjqjq44evOq6qrqurcqrqkqq6uqj+sqrts+v7fU1XvXe57YZIzNo3wFUneu/xZ5xzt7wEAzNl2iFTVZ1XVTyV5V5IXJLk6yZcneVVVVZKXJrljkocl+fwkr0pycVWdueHbnJLk+5I8MckDknxikmds+BmPSvJjSX4wyb2TvC3Jd20a5VeTPDrJLZK8vKouraof2Bw0R/gdzq+q11fV66/LNdtdAgDgGKnuvvmdqm6T5DFJnpDks5O8LMlzk7ykuz+yYb8vTvLiJLft7g9v2P6mJL/W3T9VVecl+eUkn9ndb1s+/pgkz05yand3Vf1xkjd395M2fI9XJLlbd9/5MPPdMskjkzwuyRckeU2SC5P8RndfdVO/2y3r1n2/Ovdm1wAAOHqv6Be+obsPbt6+1SMiT07y9CQfSXL37v4X3f2bGyNk6T5JTkvy/uUplauq6qok90py1w37XXMoQpbek+TkJJ+0/PweSf5k0/fe/Pn/191Xdvezu/uLktw3ye2T/FIWcQIArKkDW9zvgiTXJXl8kkuq6kVZHBH5g+6+YcN+JyR5XxZHJTa7csPH12967NBhmaO6ZqWqTsniVNBjs7h25M1JviPJRUfz/QCAnbGlF/7ufk93/3h3f0aShyS5Ksnzk7y7qn6mqj5vuesbszgacWN3X7rpf5dvY663Jrn/pm0rn9fCA6vqmVlcLPtfk1ya5D7dfe/ufnp3f2AbPxMA2GHbPgLR3X/a3d+U5MwsTtncPcmfVdUXJHlFktcmuaiqHlpVd6mqB1TVDy8f36qnJ3lCVT2pqj69qr4vyf027fPYJL+f5JZJvi7Jnbr7u7v7ku3+TgDAjK2emvkY3X1NkhcmeWFV3S7JDcsLTb8ii3e8PCvJ7bI4VfPaLC4e3er3fkFVfVqSH8/impMXJ/nZJOdt2O0PknxKd1/5sd8BANgNtvSumb3Mu2YA4Pj7eN81AwBwzAkRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGCMEAEAxggRAGDMgekBJlTV+UnOT5JTc9rwNACwf+3LIyLdfUF3H+zugyfllOlxAGDf2pchAgCsByECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGEYt//wAAADDSURBVCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIwRIgDAGCECAIyp7p6eYVRVvT/JZdNzJPnkJFdMD7FGrMcq67HKeqyyHqusx6p1WY9P7e7bbt6470NkXVTV67v74PQc68J6rLIeq6zHKuuxynqsWvf1cGoGABgjRACAMUJkfVwwPcCasR6rrMcq67HKeqyyHqvWej1cIwIAjHFEBAAYI0QAgDFCBAAYI0QAgDFCBAAY8/8AUjFAa+6XJMsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3LLCx3ZE0Ls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d699d9a3-2eeb-4077-ac38-525d1e389f43"
      },
      "source": [
        "translate(u'আমার মাথা ব্যাথা করছে।')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> আমার মাথা ব্যাথা করছে। <end>\n",
            "Predicted translation: my head aches . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2438 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2480 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2469 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2476 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2509 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2479 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2453 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2459 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2503 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2404 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2438 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2480 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2469 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2476 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2509 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2479 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2453 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2459 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2503 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2404 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUQVLVqUE1YW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "968a8950-3d2e-4c76-9b95-74034dbc5810"
      },
      "source": [
        "\n",
        "translate(u'আমি যাবো।')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> আমি যাবো। <end>\n",
            "Predicted translation: i will go . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2438 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2495 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2479 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2476 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2507 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2404 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2438 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2478 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2495 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2479 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2494 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2476 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2507 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2404 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDHBWDmCEbm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}